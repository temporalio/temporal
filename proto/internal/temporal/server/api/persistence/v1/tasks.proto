syntax = "proto3";

package temporal.server.api.persistence.v1;
option go_package = "go.temporal.io/server/api/persistence/v1;persistence";

import "google/protobuf/timestamp.proto";

import "temporal/api/common/v1/message.proto";
import "temporal/api/enums/v1/task_queue.proto";

import "temporal/server/api/clock/v1/message.proto";
import "temporal/server/api/taskqueue/v1/message.proto";

// task column
message AllocatedTaskInfo {
    TaskInfo data = 1;
    int64 task_pass = 3;
    int64 task_id = 2;
}

message TaskInfo {
    string namespace_id = 1;
    string workflow_id = 2;
    string run_id = 3;
    int64 scheduled_event_id = 4;
    google.protobuf.Timestamp create_time = 5;
    google.protobuf.Timestamp expiry_time = 6;
    temporal.server.api.clock.v1.VectorClock clock = 7;
    // How this task should be directed. (Missing means the default for
    // TaskVersionDirective, which is unversioned.)
    temporal.server.api.taskqueue.v1.TaskVersionDirective version_directive = 8;
    // Stamp field allows to differentiate between different instances of the same task
    int32 stamp = 9;
    temporal.api.common.v1.Priority priority = 10;
    // Reference to any chasm component associated with this task
    bytes component_ref = 11;
}

// task_queue column
message TaskQueueInfo {
    string namespace_id = 1;
    string name = 2;
    temporal.api.enums.v1.TaskQueueType task_type = 3;
    temporal.api.enums.v1.TaskQueueKind kind = 4;
    // After data is migrated into subqueues, this contains a copy of the ack level for subqueue 0.
    int64 ack_level = 5;
    google.protobuf.Timestamp expiry_time = 6;
    google.protobuf.Timestamp last_update_time = 7;
    // After data is migrated into subqueues, this contains a copy of the count for subqueue 0.
    int64 approximate_backlog_count = 8;

    // Subqueues contains one entry for each subqueue in this physical task queue.
    // Tasks are split into subqueues to implement priority and fairness.
    // Subqueues are indexed starting from 0, the zero subqueue is always present
    // and corresponds to the "main" queue before subqueues were introduced.
    //
    // The message at index n describes the subqueue at index n.
    //
    // Each subqueue has its own ack level and approx backlog count, but they share
    // the range id. For compatibility, ack level and backlog count for subqueue 0
    // is copied into TaskQueueInfo.
    repeated SubqueueInfo subqueues = 9;

    // For transitioning from tasks (v1) to tasks_v2 and back:
    //
    // If this TaskQueueInfo is in v1 and this is set, then v2 may have tasks.
    // If this TaskQueueInfo is in v2 and this is set, then v1 may have tasks.
    //
    // New metadata starts with this flag set (we could skip this when useNewMatcher is off).
    // Whenever locking any metadata as the inactive one (drain-only), this should be set.
    // If the flag is true, no tasks should be written to the active table until the inactive
    // table has also been locked (and the flag set there for a potential reverse transition).
    // After determinining that the inactive table has no more tasks left, then this
    // can be cleared on the active table.
    bool other_has_tasks = 10;
}

message SubqueueInfo {
    // Key is the information used by a splitting algorithm to decide which tasks should go in
    // this subqueue. It should not change after being registered in TaskQueueInfo.
    SubqueueKey key = 1;

    // The rest are mutable state for the subqueue:
    int64 ack_level = 2;
    temporal.server.api.taskqueue.v1.FairLevel fair_ack_level = 4;

    int64 approximate_backlog_count = 3;

    // Max read level keeps track of the highest task level ever written, but is only
    // maintained best-effort. Do not trust these values.
    temporal.server.api.taskqueue.v1.FairLevel fair_max_read_level = 5;

    // We can persist a limited number of fairness key counts in task queue
    // metadata so they're not lost on migration.
    repeated FairnessKeyCount top_k_fairness_counts = 6;
}

message FairnessKeyCount {
    string key = 1;
    int64 count = 2;
}

message SubqueueKey {
    // Each subqueue contains tasks from only one priority level.
    int32 priority = 1;
}

message TaskKey {
    google.protobuf.Timestamp fire_time = 1;
    int64 task_id = 2;
}
